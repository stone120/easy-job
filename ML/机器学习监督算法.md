# 机器学习监督算法

## 2. 监督算法：

标签：原理，贝叶斯，优缺点，使用场景

### 2.1 广义线性回归。

标签：最小二乘，最大似然，高斯噪声先验，基函数，后验最大化

线性回归是最简单的机器学习模型了，我们将目标值表示成特征向量的加权组合，重要的特征加权大，不重要的特征权重小。然后，我们根据目标值域预测值之间的差距，以其平方差来衡量错误的大小，然后我们将这个错误最小化（凸函数梯度为0），可以求出w的闭式解。这就是最简单的线性回归了。

但是我们肯定知道，这样简单的模型是存在问题的。第一，我们学出来的上面的模型永远是特征的线性模型，为了解决更复杂的问题，我们需要将非线性带入模型中。第二，我们为什么要用平方差来衡量误差错误？仅仅是因为这样我们可以得到一个能够有闭式解的凸优化问题吗？第三，模型的过拟合应该如何处理？

首先，我们回答第一个问题。为了解决更复杂的问题，我们可以对输入空间（特征空间）做一个变换，加上非线性的变换，例如高斯变换，sigmoid变换等等，但是整个模型仍然是w的线性模型，也就是说我们的广义线性模型是针对于w而言的。

第二个问题，我们从概率论的角度说起。我们在回归任务中，是对p(t|x)进行建模，这样根据我们的广义线性模型，我们认为目标值t服从以线性回归结果为均值，beta为精度的高斯分布（beta为噪声）。然后，我们的预测值为这个高斯分布的期望，也就是广义线性回归的结果。在优化的时候，在概率论的支撑下，我们可以以最大似然估计为准则来进行模型的学习。那么结论就要来了，在高斯噪声假设下最大似然估计的w的结果就是最小化平方差的结果，而beta的估计结果就是回归函数周围的残差方差。平方差误差函数还可以理解为预测目标与线性预测结果在欧氏空间的距离，因此最小平方差的解就是使得预测目标和线性预测目标之间的欧氏距离最短，不难证明，最近的预测值就是预测目标在特征空间的投影。

第三个问题，就是要引入在机器学习模型中占有重大比重的正则项。在现在的模型中，基本已经没有不考虑正则项的。正则项的存在意义可以从两个方向进行理解：一个是控制模型的复杂度，降低模型的vc-维；另外一个从贝叶斯角度出发，可以理解带有正则项的目标函数等于以高斯分布为先验的w的后验分布的最大似然表示(这里我们下一段再展开讲)。正则项一般可以考虑L1，L2正则，当然也有L0正则，只过限于不好优化不经常被用。L1，L2正则当然都能够起到降低模型复杂度的作用。L1正则是从稀疏模型的角度出发，由于其在损失函数等高线上，可以表示成方块的限制条件，这样在边界取极值时，一般会在坐标轴上取到，即会导致某个维度权重为0，从而有稀疏化，特征选择的作用。L2正则化是权重衰减，不想让权重很大。L2正则化在线性模型中能够让w总有闭式解，同时也因为L2是凸函数，这样的模型比较好优化，因此应用比较广泛。

接下来我们可以从贝叶斯的角度去理解回归模型。

最大后验概率模型：给w一个先验概率分布：高斯分布，均值为0，精度为alpha，预测目标t也是高斯分布，这样我们可以得到w的后验概率分布也是高斯分布。同时我们可以得到w后验概率分布的形式。这时w后验概率的最小化相当于似然函数加上正则项（参数为alpha／beta）的最小化。

那么我们在进行模型的选择的时候，一般是要对正则化系数进行交叉验证然后求解的。然而，如果我们利用贝叶斯的观点，从最大化边缘似然函数的角度出发，则完全可以由训练数据确定超参数alpha和beta。

广义线性模型的局限性：广义线性模型的局限性主要在于基函数在被观测之前就已经确定了下来，

### 2.2 logistic regression

标签：sigmoid函数，logit，beta先验，狄利克雷先验，最大熵原则，简单，精确度低，离散化特征

原理：假设目标变量服从二项分布，错误用交叉熵来衡量，不用

贝叶斯角度的原理：

为什么用的sigmoid函数与最大熵模型的关系：

优缺点：

使用场景：

为什么使用离散化特征

### 2.3 svm

标签：最大间隔，对偶，kkt条件，软间隔，SMO算法，核技术

### 2.4 核技术

有svm发展出来的核技术：

高斯过程：

### 2.3 决策树模型

标签：信息增益，ID3，C4.5，CART，信息增益比，gini系数，回归树，平方差，过拟合，剪枝。

### 2.6 随机森林

标签：bagging，投票，方差，深度，随机

### gbdt-xgboost

标签：boosting，回归树，残差，函数空间的梯度下降，一阶，二阶泰勒展开，

### 2.7 优化算法

标签：随机梯度下降，共轭梯度下降，Adam，梯度截断，ftrl

#### 2.7.1 目标问题的归一化（统一变成无约束的凸优化）

这里讲的优化主要是凸优化，也就是优化问题里面最简单的问题，机器学习中设计序诶模型的时候我们尽可能让机器学习求解的问题变成凸优化，这样比较好求解。这里，我们主要讲一下机器学习中凸优化的求解方式。

机器学习中的凸优化一般都是比较简单的，不过比较复杂的就是带有约束条件的凸优化。对于简单的凸优化，我们可以用梯度下降直接求解，(梯度下降也是有很多扩展的,下一段我们会讲到)；对于带有限制条件的凸优化，我们又可以分为两种，分别是等式约束和不等式约束，对于等式约束我们一般是采取拉格朗日乘子，通过拉格朗日系数把等式约束和目标函数合成一个式子，；对于带有不等式约束的优化，我们一般利用KKT条件，将原始问题转化为对偶问题（在KKT条件的情况下原始问题才和对偶问题的解相等）等，将问题转化为不带约束条件的凸优化进行求解。这样说完，我们肯定是有疑问的，即为什么拉格朗日乘子能有效？KKT条件是什么？什么是对偶问题？为什么一定要引入对偶问题？接下来，我就一个一个问题来回答。

首先是第一个问题，为什么拉格朗日乘子的引入能够解决等式约束的问题。我们可以想象没有约束的时候一个损失函数（前提是凸函数）在参数空间的等高线，当添加了等式约束的时候，就是另外一条曲线，我们只能取与损失函数等高线和曲线相交的点作为备选参数，然后从中抽取使损失函数最小的点。那么我们如何从相交的点中找到损失函数最小的点呢？其实仔细想想，相交意味着沿着限制条件的曲线走，以损失函数下降的方向为方向，我们仍然能够找到令损失函数比当前点小，同时还满足限制条件的点，因此按照这样的想法，直到找到损失函数等高线和限制条件相切的参数点，我们就找到了损失函数的最小值。那么这样的想法为什么和拉格朗日乘子扯上关系呢？其实，我们上面的条件，让损失函数的等高线与限制条件相切，就是让损失函数的梯度和限制条件的增长函数方向相反，于是，我们使用拉格朗日乘子，将等式约束和原始目标函数结合起来，然后对参数求导数，让导数为0，我们就可以求解到最优值。

接着，来回答我们的第二个问题，对偶问题与KKT条件。对于非等式的约束，就像等式约束，我们也需要用到拉格朗日乘子，用拉格朗日乘子将非等式约束带入方程中以后，我们同样首先来考虑，接下来我们要做怎样的动作，才能够得到与带约束条件的优化问题一致的解呢？这里的回答是，我们需要以拉格朗日乘子为参数，求解合并后的目标函数的最大值，这样得到的问题就和原始问题一致了。在这里的解释中，我们首先将所有的不等式约束都转化为小于0的约束，然后，假设有不满足限制条件的参数，即该参数使得限制条件大于0，那么我们再最大化的过程中，会把目标函数最大化至正无穷，而在后面的最小化过程中，我们就可以把这样的参数值给舍去。这样我们引入拉格朗日乘子以后，就把最一开始的带约束的求最小值问题，变成了最大最小问题。

然后就是对偶问题了，将原始的最大最小问题转换为最小最大问题，就是对偶。可是转换之后的对偶问题与原始问题的解相同吗？直观上理解的话，原始问题就是从胖子里面挑瘦子，对偶问题是瘦子里面挑胖子。这样对偶问题其实是原始问题的一个下界，只有在满足KKT条件的时候，对偶问题才与原始问题相同。KKT条件主要包括两种条件：第一种是对参数变量求导为0；第二种是满足约束已有的约束条件，拉格朗日乘子大于0；第三种是拉格朗日乘子*限制条件=0；第三种条件是最有意思的条件，我们可以由此推出支持向量的概念。

至于为什么要用对偶问题求解，原始问题不能求解吗？我认为理论上的需求大于实践中的需求，例如在SVM中，求解是利用了SMO算法。对偶问题的推导是为了得出概念上的东西，例如支持向量，而且能够引出核技术，在广义线性回归模型，logistic regression，降维算法中，我们都能够看到核技术的应用。

#### 2.7.2 梯度下降

梯度下降是目标函数求解中最基础的优化算法。我们可以从三个角度来看梯度下降的发照。首先，梯度下降是利用了目标函数的一阶导数，根据泰勒展开，我们是不是可以利用更多的二阶，甚至三阶的梯度，从而使得优化更加精确？但是，由此带来的计算上的巨大开销我们又如何避免呢？第二个角度是我们如何在只考虑一阶梯度得情况下，使得当前的下降方向更精确，而且下降速度比较快呢？第三个角度是为了避免梯度下降造成的在高维空间中的来回震荡，我们是否可以让每次的梯度下降方向都尽量无关？

梯度下降，牛顿法，海森矩阵

梯度下降，共轭梯度下降

梯度下降的改进：从两个方向思考改进思路：梯度下降的方向的修正，以及学习速率的修正。方向修正的思路就是加上动量的考虑，也就是上一次梯度的方向；学习速率的考虑主要是希望能够自适应的选择学习速率。Adagrad，Adam

#### 2.7.3 在线学习算法（ftrl）

讲解了梯度下降之后，我们再来看看online求解。在我看来，online最优化是由于各种线上应用而引起的一个需求。例如我们在预测广告点击率的时候，不能每次都线下训练一个模型，然后拿到线上跑几天，然后线下训练等等。我们希望我们的模型是变化的。而这个变化过程就可以认为是一个优化过程，随着数据的不断增加，可能会趋于稳定，也就是找到了最优解。

在具体的应用中，在线学习一般是在非常高维的空间进行学习，比较典型的就是广告点击率的预估，因此我们在保证模型精确的情况下，非常想要模型能够有稀疏的解。在线下的模型中（能够实现batch的训练），我们可以用L1正则来实现稀疏化。然而在线上，这样的L1稀疏化也是很难产生的。

综上所述，我们的在线学习的模型是由两个目标的：梯度的精确性，模型参数的稀疏性。我们可以沿着模型稀疏性的主线进行算法的介绍。

首先，为了实现参数的稀疏性，我们可以简单粗暴一些，当参数小于设定的阈值时，我们就将其置为0。但是这样在有些维度的权值没有得到足够的训练的时候，我们的截断就有可能失去某个维度的特征。既然这种截断太过于暴力，我们可以采用稍微温柔一些的方法（TG），设置两个阈值，在大于较大的阈值的时候，我们不采取任何截断，在两个阈值之间时，我们将权值的绝对值稍微降低一些，在小于较小的阈值的时候，我们让梯度等于0.可以看到，这两种方法在某些条件下是一致的（两个阈值相同时）。

有上面两者看出，在线上训练的每一次更新中，我们需要由两个步骤，第一步是求出下降的梯度方向，第二步考虑对这个梯度进行截断。在第二个步骤中，我们实际上做的是对下降方向做出一个微调。有人根据这两个步骤提出了FOBOS算法，让第二步中的微调变成一个优化问题，目标函数项包括，求解权重与利用梯度下降权重的平方差，以及一个用于稀疏的正则化项。这个优化问题，求解出来的直观理解就是，当前的样本在该维度上产生的梯度变化不够大时，我们就对该梯度进行截断。当然，这种梯度截断的方式与TG在某些条件下其实也是一致的。

上述的TG，FOBOS都是建立在SGD的框架之下的，这些算法的精度比较高，但是在稀疏性上的表现就比较一般。然后微软提出了RDA算法，将权重更新的策略表示为一个优化问题，目标函数包括三项：之前所有梯度的平均值，正则项，还有一个辅助的严格凸函数。RDA中，更新梯度的策略是党某个维度上的累积梯度的平均值绝对值小于某个阈值的时候，该维度被置为0。

稍微深入分析一下，我们能够发现在L1-FOBOS 和 L1-RDA在形式上是非常一致的，都是将权重更新问题表示成一个优化问题来求解。第一项：FOBOS中是当前的梯度下降方向，RDA考虑的是之前累加梯度的和，第二项都是L1正则项，第三项FOBOS限制权重不能离已经迭代过的太远，RDA中限制权重不能距离0太远。因此，综合以上两种算法的表示方法，提出了FTRL算法。在FTRL中，优化策略为当梯度累加与之前所有权重更新的差值小于某个阈值的话，我们对该维度进行截断。ftrl优化目标如下：

![ftrl_2](/image/ftrl_2.png)

在FTRL中，我们还必须考虑的就是学习速率的计算。考虑到特征维度的变化率，如果特征1比特征2变化更快，那么在维度1上的学习率应该下降得更快，因此我们的学习速率是与梯度的累积平方和成反比的。在ftrl的实现中，我们需要的保存的就是两个变量的值zi，thegema-i。具体更新如下图所示：

![ftrl_1](/image/ftrl_1.png)

#### 2.7.4 EM算法

##### 1.EM的介绍和证明

有一堆观测数据，在学习数据的产生规律时，在假设数据来源于同一个高斯分布时，我们能够很简单的利用最大似然（形式是log的和）对其参数进行估计。但是其实在现实生活中，我们的数据分布其实没有那么理想，很有可能是来自于多个高斯模型，每个数据产生的概率是由多个高斯模型来共同决定的，这样的模型就是混合高斯模型。这样，数据的生成概率就就是不同权重的高斯分布的和。则其log似然函数的形式就是和的log的和。而由于这种形式，即log里面有和的操作，导致我们求导比较复杂，很难做到。因此，大佬们就提出了EM这个算法，通过不断的迭代，不停的接近最优解。在实际应用中，高斯混合模型可以用在聚类。

在迭代的思想中，我们首先必须定义每次迭代得到的结果之间的关系，即如何根据上一次得到的结果估计下一次的结果？EM中，我们定义了一个隐变量Z。但是，隐变量是我们想要引入就引入的吗？统计学上是有这样的条件的，隐变量的引入不能够改变我们的观测变量的边缘分布，因此，我们这里就把隐变量的意义是说我们的数据属于哪个高斯分布的概率。那么，做了这么多工作以后，我们的EM算法到底如何工作？又有怎样的理论保证呢？

EM的证明可以从3个方向去证明（只是目前我所知道的方法），但这几种方法大同小异，都用到了Jensen’s不等式，利用凸函数的性质，证明凸函数的值的期望大于等于期望的函数值。

首先我们来看第一种证明方式，根据隐变量的含义，我们可以将似然函数（使用了观察变量的边缘概率）表达成隐变量，观测变量的联合概率与隐变量的边缘概率的表达式。然后引入一个函数Q(Z)，如下所示：

![em_1](/Users/kellydou/Desktop/em_1.png)

然后对等式两边，我们对Q(z)这个分布求期望，因为左边的表达式与Z无关，因此左边表达式不变，如下所示：

![em_2](/Users/kellydou/Desktop/em_2.png)



正如上面式子中表达的一样，后面那项是分布Q(Z)与p(Z|x,theta)的KL散度。这样，忽略到后面一项，我们就找到了似然函数的下限。这样EM算法中的两步：E步，固定theta，使得Q(Z)等于p(Z|x,theta)，这样KL散度测量就为0，我们就找到了这个下界河目标函数相交的点，然后在M步，我们固定Q(Z)，求得新的theta，M步需要优化的目标函数如下。后面的那项跟新的theta无关，因此我们只要根据前面的Q函数优化就好。![em_4](/Users/kellydou/Desktop/em_4.png)

第二种证明表示出边缘概率似然函数的积分形式，然后利用jensen‘s不等式：

![em_3](/Users/kellydou/Desktop/em_3.png)

之后求解的方式与上面一样。

第三种证明其收敛性的方式叫做“Tagare” approach，在徐亦达老师的视频里面，这个方法讲的很好。要证明theta在收敛，我们需要证明第t+1次的似然函数比t次的似然函数要大。我们首先将似然函数表达出来，同样也是用z这个隐变量将似然函数表达成两个部分，然后在等式两边分别求分布p(Z|X,THETAold)的期望。

![em_5](/Users/kellydou/Desktop/em_5.png)



我们可以看到似然函数可以表示成两个部分Q，H，如果我们能够证明将t+1次的theta带进去的时候，Q变大，而H变小，就可以证明EM的收敛性。因此一方面，我们通过对Q求最大值得到t+1时刻的theta，这样Q肯定是变大的，另一方面，我们可以根据jensen‘s不等式证明H是变小的，至此，EM的收敛性得证。 

可能证明写的不是很清楚，可以参考徐老师的视频，以及https://jiqiujia.github.io/2016/09/03/Expectation-Maximization-Algorithm/。

##### 2.EM算法的应用（有隐变量的模型）

混合朴素贝叶斯模型，高斯混合模型，概率PCA，因子分析，pLSA模型等等。

### 2.9 HMM算法

### 2.10 条件随机场