# 项目相关准备

## 1. cvr预估比赛

### 1.1 数据清洗

稀疏点：在别人的比赛里，看到别人处理了出现次数比较少的点。点击次数少于1000次的，进行标记。万分点的提升。

错误点：对30号的数据进行错误纠正，去掉平均转化率大于1天的app的数据。

### 1.2 特征工程

#### 1.2.1 所用到的特征

1. 将提供给的信息分类：分为用户特征，app特征，广告特征，上下文特征，还有组合特征，分别对这些特征进行组合，然后求其计数，点击率，转化率，排行，标记等特征。

2. ctr计算平滑有两种方法的平滑：数据层级结构的贝叶斯平滑，以及时间窗口的贝叶斯平滑：

   （1）时间窗口的平滑：统计每天的ctr率时，不仅与当天的ctr相关，也与昨天的ctr相关。以gamma为系数。

   （2）数据层级结构的平滑（只对position这个特征做了平滑）：数据层级的贝叶斯平滑可以这样理解：同一个position下面的Ad，每个Ad的点击服从一个二项分布，但是她们又是在同一个position下面的Ad，它们的点击不光是由于自身的性质，而且收到position的影响，这个position对他们的影响是一致的，服从beta分布，里面有alpha，beta两个参数需要我们求解，或者说设置。

   而求解这个beta分布里面的参数，我们用到的方法是最大边缘似然概率，边缘似然概率指的是观测变量的概率分布，我们可以画出这样的概率图：

   ![ctr_1](/Users/kellydou/Desktop/ctr_1.png)

   这样我们的观测变量就可以表示成积分的形式，然后将似然函数对alpha，beta求导，就可以得到alpha，beta的迭代形式。

   ![ctr_2](/Users/kellydou/Desktop/ctr_2.png)

#### 1.2.2 特征选择方法

### 1.3 模型简介

在广告预估中，我们用的比较多的是简单的lr模型，但是这种方法需要大量的特征工程，因为lr模型只能够得到模型的一阶特征，组合特征必须要人为的进行特征工程。为了能够得到模型的二阶特征，甚至是高阶特征，大神们就想出了其他的各种模型，例如fm，能够学到二阶特征，例如dnn，能够学到模型的高阶特征。下面这个表格就展示了各种学习模型的融合。

|       模型       | 组合方式                 |             输入层             |                embedding层                |                  Deep部分                  |
| :------------: | -------------------- | :-------------------------: | :--------------------------------------: | :--------------------------------------: |
|   wide&deep    | 线性层+ deep层           | sparse features：one-hot，离散化 |                  指定大小的                   | 所有field特征都one-hot后再经由一个embedding层后得到一个词向量 |
|      PNN       | Deep层                |                             | 用FM的二阶权重做输入层到embedding的权重，embedding向量的大小为field-num * dim | <u>**deep的第一层与product层相连，product层包括线性，两两内积，偏置**</u> |
|     DeepFM     | **<u>线性层+Deep层</u>** |                             | 用FM的二阶权重做输入层到embedding的权重，embedding向量的大小为field-num * dim |            deep层从embedding层学习            |
| ***FFM&Deep*** | ***线性层+Deep层***      |                             |         ***用FFM形成embedding矩阵***          | ***deep的第一层与product层相连，product层包括线性，两两内积，偏置*** |

在用dnn与线性模型做融合的时候，我们需要注意的就是embediing层，在这样的输入中，输入的都是稀疏向量。在embedding层，我们将输入的稀疏向量转化为比较密集的向量或者是矩阵。例如，我们可以利用fm的转换将输入转化为filed-cnt * dim的向量，也可以利用ffm的思想将输入转化为feild-cnt * feild-cnt * dim的矩阵。

### 1.3 模型训练

| 模型                             | 数据前期处理                                   | 模型使用                                     | 效果                                       |
| :----------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| 非线性模型：使用连续特征，xgboost ，lightGBM | 将原始特征使用历史点击率进行代替，去掉原始特征                  | 1.xgb中使用类别特征必须要对类别特征进行编码，要把类别特征转化为连续特征，但是lightGBM是可以处理离散特征的，这和在xgb中使用one-hot编码的效果是一样的                             2. xgboost和lightGBM都是基于gbdt的，然后由于xgboost训练速度太慢，我们就用训练速度比较快的lightGBM做特征工程，然后将训练出来的特征。 | 单模型0.103左右                               |
| 线性模型：使用离散特征，ffm                | 将比率等连续特征使用log，然后求区间进行离散化处；还可以使用xgboost预先训练进行 | ffm中比较重要的调参就是隐变量参数，正则系数，还有学习速率；我们在这次比赛中只调节了隐变量参数，和正则项。在训练过程中发现ffm很容易过拟合，因此我们把正则项设置的很大，然后把学习速率调节的非常小。 | 单模型0.103～0.104之间                         |
| 线性模型     ftrl                  | 数据离散化处理：将所有的数字特征用xgboost做离散化处理，然后使用原始id特征和xgb叶子特征进行lr训练 | 使用了组合id特征，原始id特征，还有gbdt的叶子特征，但是模型        | 模型训练出来，差距比较大，并且预测结果概率偏低。                 |
| 非线性模型，神经网络模型：wide&deep模型       | 线性部分输入：离散特征，组合特征；deep部分输入：组合特征，连续特征，     | deep部分的embedding层有两种embedding的方法，一种是用一个长向量对所有的输入进行embedding，只有一个词向量，另一种方法是对每个field有一个词向量，针对这个想法，我们可以把fm以及ffm的参数用来做得到词向量的权重，这也是后来deep ffm模型的重要参考之一。 | 模型训练出来是0.104左右，，，可以使用fm或者ffm模型对embedding层进行简化 |

面试中遇到的问题：

1. lr为什么使用离散特征会好一些？

   1. 离散特征的增加和减少都很容易，易于模型的快速迭代；
   2. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
   3. 离散化后的特征对异常数据有很强的鲁棒性，模型会更稳定：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
   4. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
   5. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
   6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。

2. Ftrl的原理，实践表现？为什么表现不好？

   http://www.datakit.cn/blog/2016/05/11/ftrl.html#工程tricks

   ftrl是将w的梯度求解过程转化成了一个优化算法，优化目标函数中包括：该方向上的梯度的累积和做稀疏的一次项，以及一个L1，L2正则项，还包括每次迭代过程不能差距太大，评估所求W与前t次W的距离和。

   其中的学习速率也是自适应的，每一次的学习速率于梯度的累积平方和成反比。

   在ftrl的实现中，除了更新每个维度上的w，我们还有两个变量需要保存，一个是z，用来保存梯度的累积和-以及之前该维度上的w；还有一个变量n，用来保存梯度的下降累积。每次迭代时，先计算g与z的更新，然后根据z的大小来更新w。

   表现不好：特征只是用了原始特征和组合特征，而且没有调参，，其中的alpha，beta，L1，L2正则系数都没有调整。

   beta的参数不用调整，但是初始学习速率的alpha 设置为0.1。

3. ffm模型怎样于deep模型进行融合？有怎样的理论基础？效果怎么样？

   ffm于deep模型的融合是将整个模型分成三个部分，有线性部分，ffm部分，deep部分；这三个层共享输入，输入数据是filed：feature：value这样的模式。

   在线性部分，直接对数据进行one-hot编码，然后设置权重；

   在ffm部分和deep部分共享一个embedding层，设置w为field_cnt * field_cnt *dim, 然后每一个field对应一个embedding的矩阵，矩阵大小为field_cnt * dim；

   ffm部分，直接输出加上，deep部分连接上全链接层。

   最后经三个结果进行叠加，再通过sigmoid激活函数输出。

   效果：只用了原始特征，线下 0.103左右，0.105～0.106，我记得lr的baseline是0.107还是0.108来着。。。

   调参：使用了3层神经网络，128，64层。使用relu激活函数，用了3个epoch，每次epoch对数据进行random，然后3次epoch求平均，batch size 是 1000，因为只使用了2天的数据进行训练。

4. 有哪些可以改进的地方？

   一是特征工程上的优化

   二是模型上的优化，deep层还有很多可以防止过拟合的方法没有使用。

   三是增加数据，使用最近7天的数据进行训练，可能数据增加到一定程度以后再增加也没有意义了。实现数据的并行处理和学习。

### 1.4 模型融合

将两个结果比较接近的模型进行了融合：xgb结果，ffm结果，还有wide&deep的模型结果。

### 1.5 问题存在

1. 特征工程中还有一些想法：
2. 训练出来的ftrl模型效果很差：
3. 不同的模型有不同的特性，对不同种类的特征得到的结果不一样，在模型融合的时候不能仅仅是模型融合的过程，还要考虑不同的模型使用不同的特征
   1. 例如lr比较适合用离散的特征，这时候就不能将那些连续的数字特征直接放到模型里面，而是应该先采取离散化的方式，例如log处理，
   2. xgboost处理离散特征需要one-hot。
   3. ftrl训练时间太长，因为是一个数据一个数据进行训练的。但是因为每一次w的更新都是基于上一个w的迭代结果。好像google的那篇论文里面实现了
4. 没有探索使用deep模型 + ffm 模型

## 2 kaggle 比赛项目

3.2.1 自然语言处理相关算法

分词算法,jieba

主题模型相关算法

## 3.机器学习实践技巧

4.1 数据异常点检测

异常点检测处理方式与聚类很类似，我们

4.2 数据归一化的重要性

4.2 数据过拟合处理方式

4.3 特征工程，特征选择技巧

4.4 MapReduce 以及 spark技巧

