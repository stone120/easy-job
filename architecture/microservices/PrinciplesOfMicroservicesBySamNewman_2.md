# 第六章 部署
1. 持续集成（CI）。好处：能够得到关于代码质量的某种程度的快速反馈；自动生成二进制文件；代码在版本控制之下，需要的话可以重新生成某个版本的构建物；可以从部署的构建物回溯到对应版本的代码；有些CI工具可以使运行过的测试可视化。

是否真正理解CI的三个问题：你是否每天签入代码到主线？你是否有一组测试来验证修改？当构建失败后，团队是否把修复CI当作第一优先级的事情来做？

2. 把持续集成映射到微服务

微服务、CI构建、源代码三者的关系如何？分步骤开始：a. 所有代码在一个代码库、只有一个CI构建，生成多个微服务。缺点：修改某服务的一行代码，所有服务代码都需要重新构建和验证，浪费时间和资源，更会导致不知道哪些构建物要重新部署。b. 还是一个代码库，但是多个CI分别映射代码库的不同部分。优点：简化检出/检入流程。缺点：会觉得同时提交多个服务修改很简单，做出将多个服务耦合在一起的修改。c. 每个微服务都有自己的代码库和CI，微服务相关的测试应该和其本身的代码放在一起。优点：修改某个微服务后可以快速验证，修改代码只需要做该微服务的构建和测试，代码库与团队的匹配度更高。

3. 构建流水线和持续交付

流水线第一阶段运行快速测试，第二阶段运行耗时测试。在某个阶段失败，能够快速反馈，不用进行后面阶段的测试，节省时间。越接近后面阶段，越能在生产环境下工作。持续交付（CD）检查每次提交是否达到了部署到生产环境的要求，并持续反馈信息。一个示例流水线的几个阶段：编译及快速测试、耗时测试、用户验收测试、性能测试、生产环境。不要对CI工具扩展来做CD，会导致复杂化，要使用为CD设计的工具。

如果在团队初始阶段，没有很好识别出服务边界，可以将所有代码放在一个库中，所有服务放在一个构建，减轻跨服务修改所带来的代价。

4. 平台特定的构建物

各语言有自己的构建物类型和构建工具，Ruby有gem，Java有JAR包或WAR包，Python有egg。对微服务部署来说，还需要一些其它工具。Ruby和Python需要运行在Apache或Nginx中的进程管理器。为了部署和启动，需要Puppet、Chef和Ansible这样的自动化配置管理工具，它们支持多种技术栈（编程语言）的构建物。

5. 操作系统构建物

可以使用操作系统支持的构建物来取代不同技术栈的构建物避免部署的复杂性。Redhat或CentOS使用RPM，Ubuntu使用deb包，Windows使用MSI。优点：不用考虑底层技术，使用内置工具即可完成软件安装。有些OS包管理器可以完成Chef或Puppet的工作。缺点：刚开始写构建脚本会比较困难。Linux下的FPM包管理器功能比较完善，Windows的MSI原生打包系统差了不少，NuGet好一些，Chocolatey NuGet提供的功能和Linux上的很接近了。另外，如果多种操作系统部署，代价也就高了。

6. 定制化镜像

使用自动化配置管理工具，安装环境仍然耗时耗力。一种方法是创建虚拟机镜像，其中包含常用的依赖。优点：减少装环境的时间，缺点：构建镜像花时间长，镜像文件大，例如20GB。构建不同镜像工具链不同，AWS AMI、Vagrant镜像、Rackspace镜像。Packer可以简化创建过程，可以支持多种镜像。

将镜像作为构建物。不仅将环境部署在镜像中，将服务也部署进去。

不可变服务器。部署完成后，如果有人登录上去对机器做一些修改，可能导致实际配置与源代码中的配置不一致，导致配置漂移。可以在镜像创建过程中禁止SSH，确保没人能登录修改。

7. 环境。要重视测试环境和生产环境的差异性，要在测试环境尽可能接近生产环境和消耗的人力物力之间做出权衡。

8. 服务配置。服务配置的工作量应该很小，仅仅局限于环境间的不同之处。如果配置修改了服务很多基本行为，或者环境之间配置差异很大，有可能在某个环境中出现特定问题。如何处理不同环境之间的配置差异：a. 每个环境创建一个构建物，配置内建于构建物中。但这样对于测试环境生成的构建物是无法保证在生产环境能正常运行的。b. 更好的方法是只创建一个构建物，将配置单独管理。可以用专用系统来提供配置。

9. 服务与主机之间的 映射

一个主机可以运行多少服务？这里的主机（host）是指运行独立操作系统的隔离单元。没有虚拟化的话，一个主机对应的是一台物理机，采用虚拟化的话，对应的就是一个虚拟机。

单主机多服务。优点：管理主机工作量小，硬件成本低，简化开发人员工作。缺点：监控服务占用资源困难，服务之间互相影响，单一服务负载过高可能影响其它服务正常运行；服务部署困难，每个服务可能依赖不同的环境；不利于团队自治，可能需要独立团队来管理主机配置，增加协调工作；限制部署构建物的选择；增加单个服务进行扩展的复杂性，每个服务对于主机的需求未必一致。

应用程序容器。可以利用IIS的.NET应用程序部署或基于servlet容器的Java应用程序部署，将不同的服务放在同一个容器中，再把容器放在单台主机。优点：简化了管理，对多实例提供集群支持、监控等；节省语言运行时开销，因为多个服务泡在一个JVM上了。缺点：限制了技术栈的使用；其在内存中共享会话状态的方式对于微服务来说应该避免，限制了服务伸缩性；容器启动时间很长；在类似于JVM的平台上，多个应用程序处在一个进程中，分析资源使用和生命周期管理都很困难。建议：对应技术栈的自包含的微服务构建物。如.NET中的Nancy。以及Jetty嵌入式容器中就包含了非常轻量级的HTTP服务器。这样能够保证伸缩性。

每个主机一个服务。优点：避免了单主机多服务的很多问题，简化了监控和错误恢复；减少潜在的单点故障；对某一服务扩展容易；支持不同部署方式如镜像部署或不可变服务器。缺点：管理服务器工作量增加。

平台即服务（PaaS）。PaaS的工作层次比单个主机高，往往依赖于特定技术的构建物，如Java WAR包或Ruby gem等。优点：还能帮助自动配置运行，有的能透明地进行系统伸缩管理。Heroku是一个很好的PaaS平台，能管理服务并以简单的方式提供数据库等服务。缺点：出问题时解决起来较困难。越想根据应用程序使用情况来自动收缩，越难以做好。平台尽量满足的是通用需求，对于应用的特定需求，往往难以满足。

10. 自动化

软件部署、服务监控、进程查看和日志收集等工作在服务规模大的时候，应该靠自动化来解决。理想情况，开发人员使用的工具链要和部署生产环境时使用的完全一样，这样能及早发现问题。自动化能显著提高微服务的开发和部署效率。

11. 从物理机到虚拟机

管理大量主机的关键之一是，找到一些方法把物理机划分成小块。

传统的虚拟化技术。标准的虚拟化技术包括AWS、VMWare、VSphere、Xen和KVM，其架构从底层到最上层是：机器、内核、主机操作系统、hypervisor、虚拟机（又包括内核、操作系统和应用）。其中的虚拟机可以安装不同的操作系统，可被认为完全封闭的机器。Hypervisor会占用CPU、I/O和内存等资源，开的虚拟机越多，hypervisor占用的资源就越多。因此，物理机切分的越来越小的时候，收益也越小。

Vagrant。是一个部署平台，通常在开发和测试环境使用，而非生产环境。能帮助在本地机器上轻松创建类生产环境，可以同时创建多个VM，通过关闭几个测试故障模式。缺点：开发机上会有更多的资源消耗。

Linux容器。Linux容器可以创建一个隔离的进程空间，进而在这个空间中运行其他的进程。最流行的是LXC。其架构从底层到最上层是：机器、内核、主机操作系统、容器（包含操作系统和应用）。容器运行的操作系统必须要和主机操作系统相同的内核。没有了hypervisor，启动速度更快。相同硬件上能比VM运行更多的数量的容器，资源利用更高效。容器也能很好地在虚拟机上工作。缺点：需要花费工作让外界看到一台主机上的不同容器，并讲外部请求路由到容器。容器的隔离性没有VM好，某些容器进程可能会跳出容器，与其它容器进程或底层系统发生干扰。

Docker。构建在容器上的平台，进行容器管理，可以创建和部署应用。可以在Vagrant中启动单个VM，其中运行多个Docker实例，每个实例包含一个服务。这样在单机上开发和测试更便捷和省资源。CoreOS是专为Docker设计的操作系统，占用资源更少。Google的开源工具Kubernetes和CoreOS集群能进行跨集群的Docker管理和调度。有个工具Deis（http://deis.io/），试图像Heroku那样在Docker上提供PaaS。

12. 一个部署接口

参数化的命令行调用是触发任何部署的最合理的方式。需要包含微服务名字、版本和要部署的环境。Python库Fabric可以讲命令行调用映射到函数，也提供类似SSH的机制控制远程机器。Ruby可以用Capistrano，Windows可以用PowerShell。

环境定义。对微服务配置，完成微服务到计算、网络和存储资源之间的映射。可用YAML文件描述。例子中配置的资源包括：开发和生产环境下不同的节点名称、资源大小、凭证（credential），服务以及节点个数；微服务中定义了运行的Puppet文件名称、连接属性（tcp，端口号，允许范围）。构建定义系统工作量很大，Hashicorp有个工具Terraform可以帮助做这些事情。

13. 小结。服务要能够独立于其它服务部署。每个服务放到单独的主机/容器中。自动化一切。推荐Jez Humble和David Farley的《持续交付》

# 第七章 测试
1. 测试类型

面向技术的测试：单元测试、非功能性测试（响应时间、可扩展性、性能测试、安全测试）。面向业务的测试：验收测试、探索性测试（可用性测试、我如何破坏系统功能）。对于微服务，要尽可能扩大自动化测试范围。

2. 测试范围

Mike Cohn的《Scrum敏捷软件开发》提出了测试金字塔，底层是单元测试，中间是服务测试，上层是用户界面测试（这里称为端到端测试）。

单元测试。通常只测试一个函数和方法调用，面向技术而不是业务，能快速反馈功能是否正常，对代码重构非常重要。

服务测试。绕开用户界面，直接对服务进行测试。需要给所有外部合作者打桩。

端到端测试。模仿用户使用的测试。

权衡。三种测试在测试范围、运行时间、反馈周期、定位范围都不同，要为不同目的选择不同测试来覆盖。

比例。好的经验：下面一层的测试的数量要比上面一层的多一个数量级。

3. 实现服务测试

mock还是打桩。打桩，是指为被测服务的请求创建一些有着预设响应的打桩服务。mock，与打桩相比，mock还会进一步验证请求本身是否被正确调用，需要创建更智能的合作者。但过度使用mock会让测试变得脆弱。关于二者的权衡，可以参考弗里曼和普雷斯的书《测试驱动的面向对象软件开发》。

智能的打桩服务。Mountebank是一个打桩/mock服务器，可以避免很多重复工作。

4. 端到端测试

让多个流水线扇入到一个独立的端到端测试的阶段。也就是各个服务有自己的构建、单元测试和服务测试，但没有自己的端到端测试，所有服务都共享最后的端到端测试。

5. 端到端测试的缺点。有很多缺点（详见下面章节）。

6. 脆弱的测试

端到端测试由于依赖太多的因素，比如其它服务、网络等等，很可能出现失败时并不是由被测的那个服务所导致的。这就导致测试变得脆弱。Martin Fowler建议当出现脆弱测试时，如果不能立即修复，就将其从测试套件中移除。另外，还可以尝试用小范围测试替代脆弱测试。

谁来写这些测试。测试由所有人来写，会导致测试用例爆炸，测试失败后，都认为是别人的问题。由专门的团队来写，开发人员远离测试代码，周期变长，并且难以理解和修复测试中的问题。好的平衡在于共享端到端测试套件的代码权，对测试套件联合负责。

测试多长时间。很多端到端测试时间很长，有的要一天，有的甚至六个星期。并行测试工具如Selenium Grid可以缓解该问题。但重要的在于权衡哪些测试需要哪些可以抛弃。这很困难。

大量的堆积。测试周期长导致部署等待时间长，部署次数少，代码变更就会不断累积。因此要尽可能频繁地发布小范围的修改。

元版本。不要追求一起部署服务，以及使用相同的版本号，这会导致失去服务单独部署的能力，并且导致服务之间的耦合。

7. 测试场景，而不是故事

针对少量的核心场景进行端到端测试，避免针对庞大规模服务的端到端测试。但更好的方法可能是下面的CDC测试。

8. 拯救消费者驱动的测试

CDC(Consumer-Driven Contract)消费者驱动的契约，可以不需要使用真正的消费者也能确保新的部署不会破坏给消费者的服务。CDC测试的层次与服务测试的层次一样，都在金字塔中间，但其更侧重消费者如何使用服务。

Pact。Pact是一个开源的消费者驱动的测试工具，Ruby语言开发，支持JVM和.NET版本。Pacto是另外一个工具，名字很相近。

关于沟通。CDC需要消费者和生产服务之间有良好的沟通。

9. 还应该使用端到端测试吗

CDC可以替代端到端测试，但在语义监控进行生产系统监控时，还是会用到端到端测试。端到端测试可以在你需要的时候使用，比如尚未构建好CDC测试，完成后可以减少。

10. 部署后再测试

区分部署和上线。部署在生产环境，在真正负载之前进行测试，有助于发现特定环境的问题。还可以使用蓝/绿部署，部署两份软件，但只有一个接受真正的请求。蓝/绿部署需要能够切换生产流量到不同的主机，需要提供足够多的主机支持两个版本部署。蓝/绿部署甚至可以做到零宕机部署。

金丝雀发布。金丝雀发布是指通过将部分生产流量引流到新部署的系统，来验证系统执行情况。与蓝/绿发布不同的是，新旧版本共存的时间更长，而且经常会调整流量。

平均修复时间（MTTR）胜过平均故障间隔时间（MTBF）。要好好考虑如何监控以及从故障中恢复过来，而不仅仅是考虑充分的测试。

11. 跨功能的测试

跨功能需求（Cross-Functional Requirement, CFR）比非功能需求更好地涵盖了一个事实：这些系统行为仅仅是许多横切工作融合的结果。跨功能需求包括数据的持久性、服务的可用性、吞吐量和服务可接受的延迟等方面。CFR测试也是金字塔分层的。建议尽早去看CFR，并定期审查。

性能测试。微服务使得跨网络边界的次数增多，因此追踪延迟的根源很重要。需要一个类似生产的数据量，更多的机器。性能测试运行时间长，每次构建的时候运行性能测试并不可行。可以每天运行一个子集，每周运行一个更大的集合。还是要尽可能频繁。

12. 小结

使用不同类型测试，反馈要迅速；尽量使用CDC测试来替代端到端测试；使用CDC测试提供团队之间的对话要点；在努力测试与更快地在生产环境中发现并修复问题之间找到平衡

# 第八章 监控


微服务系统出了问题，要通过服务的监控、日志的筛选、网络延迟的判断等各方面去发现问题，要处理的点很多，该如何办？答案是：监控小的服务，然后聚合起来看整体。

1. 单一服务，单一服务器

首先监控主机：CPU、内存等。可以使用监控软件Nagios或者像New Relic这样的托管服务来帮助监控主机。接着查看服务器日志，可以使用命令行工具扫描日志，使用logrotate移除旧的日志。最后监控应用程序本身，至少要监控服务响应时间。可以通过查看运行服务的Web服务器或者服务日志完成。进一步可以追踪报告中错误出现的次数。

2. 单一服务，多个服务器

仍然要监控每个主机的资源使用情况，需要聚合各主机的信息来分析，也需要对单个主机信息进行深入分析。可以采用Nagios。对于日志，如果只有几个主机，可以用像ssh-multiplexers这样的工具，在多个主机上运行相同的命令。用一个大显示屏，运行grep “Error” app.log来定位错误。对于响应时间，可以在负载均衡器中跟踪，负载均衡器本身也需要跟踪。

3. 多个服务，多个服务器

通过日志和应用程序指标的集中收集和聚合来定位问题。

4. 日志，日志，更多的日志

可以用logstash，解析多种日志格式，发送到下游系统。Kibana是基于ElasticSearch查看日志的系统。

5. 多个服务的指标跟踪

需要长时间收集系统运行指标，以了解其模式，从而判断异常。可以用Graphite来方便地从新的主机收集指标，查看聚合后的数据。

6. 服务指标

Linux上安装collectd并指向Graphite时，会有大量的指标。像Nginx或Varnish这样的支撑子系统，也会提供很多信息，如响应时间、缓存命中率。对于应用程序，强烈建议公开自己服务的基本标准。这样可以：了解系统各个功能的使用情况；了解用户如何使用我们的系统，从而得知如何改进；我们永远不知道哪些数据是有用的，因此要暴露一切数据，通过指标系统来处理。Codahale的Metrics库（运行于JVM）可以存储指标，并能将数据发送给Graphite。

7. 综合监控

对于系统服务的监控，可以采用合成事务的方式，确保系统行为在语义上的正确性，这种技术因此常被称为语义监控。创建假事件给系统处理，监控处理行为就是一个合成事务的例子。

实现语义监控。可以采用针对指定服务或整个系统的端到端测试进行语义监控。但要确保测试的数据和实时的数据相匹配，还要确保不会产生副作用。

8. 关联标识

要能够像查看堆栈一样查看由请求引起的调用链。可以使用关联标识（ID），在触发第一个调用时，生成一个GUID，然后将其传递给所有的后续调用，日志中保存该关联标识，就能通过查看日志进行跟踪。每个服务都应知道传递关联标识。可用Zipkin进行跨多个系统边界跟踪调用。Zipkin有点重，需要自定义客户端并且支持收集系统。传递关联标识需要在所有服务中保持一致，可以通过统一的库来进行，实现该库时需要尽量减少其依赖。

9. 级联

级联故障很危险，监控系统之间的集成点非常关键。每个服务应该追踪和显示其下游服务的健康状态，然后将这些信息汇总，整合到一个画面。可以用库实现一个断路器网络调用，帮助你更优雅地处理级联故障和功能降级。如JVM上的Hystrix，提供了挺好的监控功能。

10. 标准化

监控领域的标准化很关键，可以利用工具，例如提供预配置的虚拟机镜像，镜像内置logstash和collectd，还有一个公用的应用程序库，使其与Graphite容易交互。

11. 考虑受众

不同的人对于数据进行深入分析的需求不一样。需要考虑：他们现在需要知道什么，他们之后想要什么，他们如何消费数据。定量信息的图形化显示可以参考Stephen Few的《Information Dashboard Design: Displaying Data for At-a-Glance Monitoring》一书。

12. 未来

存储业务指标的系统通常无法直接、实时地访问，而存储运营指标的系统却可以。如果能有通用的事件路由系统，使两者能聚合用于生成报告，则整体架构会更加简单。Riemann是一个事件服务器，允许高级的聚合和事件路由。Netflix开源的Suro类似。聚合的数据可以分发到不同的系统，如Storm、Hadoop或Kibana。

13. 总结

对每个服务：跟踪请求响应时间、错误率和应用程序级指标；跟踪所有下游服务的健康状态，如调用时间、错误率；标准化如何收集和存储指标；以标准格式讲日志记录到一个标准位置；监控底层操作系统。

对系统：聚合CPU等主机层级的指标和程序级指标；确保指标存储工具可以在系统和服务级别做聚合，也能查看单台主机信息；指标存储工具允许维护数据足够长时间，以了解趋势；使用单个可查询工具对日志进行聚合和存储；强烈考虑标准化关联标识的使用；了解什么样的情况需要行动，并构造警报和仪表盘；调查对各种指标聚合和统一化的可能性。

更多通用事件处理系统的内容可参考作者的书《Lightweight Systems for Realtime Monitoring》

# 第九章 安全
1. 身份验证和授权

身份验证是确认他是谁的过程。对要验证的人或事，称之为主体。授权机制使得能够将主体映射到他可以进行的操作中，即他能做什么，不能做什么。

常见的单点登录实现。单点登录（Single Sign-On, SSO），在企业级领域占据统治地位的是SAML和OpenID Connect。SAML是基于SOAP的标准，使用复杂，OpenID Connect已成为OAuth 2.0具体实现的一个标准，在互联网可以用Google的外部身份提供者，但在内网则缺少身份提供者。OpenAM和Gluu也是选择。

单点登录网关。使用单点登录网关作为代理与身份提供者握手，避免每个服务都要实现与身份管理者握手的功能。Shibboleth工具和Apache一起用，能很好处理与基于SAML的身份提供者的集成。不要将所有的安全措施放在网关上（所有鸡蛋同一个篮子），不要将太多功能由网关实现（高耦合点，受攻击面越大）。

细粒度的授权。通过判定更细的角色或用户组，进行细粒度授权。这样能够减少微服务本身的处理工作量。但不要过于细，会增大系统维护的工作量。粗粒度与细粒度的权衡要结合组织架构考虑。

2. 服务间的身份验证和授权

在边界内允许一切。在边界内对服务的任何调用都是默认可信的。优点：简单。缺点：难以应对中间人攻击等入侵了网络并渗透进系统的攻击。这不是一个好的方式。

HTTP(S)基本身份验证。允许客户端在标准的HTTP头中发送用户和密码，服务端验证确定客户端权限。优点：容易理解，支持广泛。缺点：HTTP有风险，HTTPS更安全；服务端需要管理SSL证书；SSL上的流量不能被反响代理服务器（如Varnish或Squid）所缓存，需要自己做；如果已经用SSO方案，融合带来工作量；无法确认请求来源。

使用SAML或OpenID Connect。优点：有现成的解决方案，可以进行细粒度身份验证。缺点：要考虑如何安全存储凭证；实现时代码工作量较大。

客户端证书。TSL（Transport Layer Security， 安全传输层协议）是SSL在客户端证书方面的继任者，每个客户端都安装一个X.509证书。缺点：证书管理工作更加繁重。

HTTP之上的HMAC。基于哈希的消息码（Hash-based Message Authentication Code）对请求进行签名，是OAuth规范的一部分，被广泛用于AWS的S3 API。优点：可检测消息被篡改；不会发送私钥；通信能被缓存。缺点：客户端和服务端需要找到好的方式交流密钥；缺乏一个优秀、开放且有效的实现方式；

API密钥。允许服务识别出是谁在调用，然后针对性地限制。对程序比SAML更易用。解决方案在开源和商业领域有很多选项。

代理问题。小心混淆代理人问题。

3. 静态数据的安全

深度防御很重要。记住基本的原则。

使用众所周知的加密算法。静态数据加密，可以选择已实现的AES-128或AES-256。密码使用加盐密码哈希技术。

一切皆与密钥相关。密钥的存储管理，可以选择单独的安全设备来加密和解密数据，或使用单独的密钥库，或数据库内置的加密支持（如SQL Server的透明数据加密）。不要自己实现方案，而是研究和使用已有的方案。

选择你的目标。仔细研究加密哪些数据。

按需解密。第一次看到数据就加密，需要时才解密。

加密备份。

4. 深度防御

防火墙。如ModSecurity。

日志。可帮助检测发生了不好的事情，敏感信息不要存在日志中。

入侵检测（和预防）系统。

网络隔离。不同服务可以放在不同网段，通过VPN隔离，定义互连规则（peering
rules）。

操作系统。打补丁自动化，微软的SCCM或红帽的Spacewalk工具。查看Linux本身安全模块的发展，如AppArmour、SELinux以及GrSSecurity这三个工具。

5. 一个示例

对于不同的请求源使用不同的安全措施，如浏览器使用HTTP&HTTPS，第三方消费者使用API密钥，第三方版税系统使用客户证书。服务与外部使用网络防御，服务之间使用HTTP，客户数据进行加密。

6. 保持节俭

只存储完成业务运营或满足当地法律所需要的信息，德语Datensparsamkeit。

7. 人的因素

8． 黄金法则

不要实现自己的加密算法，不要发明自己的安全协议。

9. 内建安全

熟悉OWASP十大列表和OWASP的安全测试框架。采用帮助探测系统漏洞的自动化工具。ZAP（Zed Attack Proxy）：重现对网站的恶意攻击；针对Ruby的Brakeman：通过静态分析发现导致漏洞的编码错误。可以参考微软的安全开发生命周期（Security Development Lifecycle）中的一些模型。

10. 外部验证

找外部团队进行安全评估。

11. 小结

识别风险级别，采用不同的方案。深度防御很重要。基于浏览器的应用程序安全的概念可以参考OWASP（Open Web Application Security Projects）。密码学可参考Niels Ferguson、Bruce Schneier和Tadayoshi Kohno所写的Cryptograph Engineering。不要忽视人的因素